\documentclass[a4paper, 11pt, titlepage]{article}
%\usepackage{color}
%\definecolor{light-gray}{gray}{0.95}

\usepackage{xcolor}
\usepackage{alltt}
\usepackage{url}
\usepackage{tikz}
\usepackage{ulem}
\usepackage{setspace}
\usetikzlibrary{trees}

\newcommand
{\image}[2]{\vspace{10 mm} \includegraphics[width=\textwidth]{#1}
\begin{center} \caption{#2} \end{center}
\vspace{10 mm}
}

% Command for inserting a todo item
%http://midtiby.blogspot.ie/2007/09/todo-notes-in-latex.html
\newcommand{\todo}[1]{%
% Add to todo list
\addcontentsline{tdo}{todo}{\protect{#1}}%
%
\begin{tikzpicture}[remember picture, baseline=-0.75ex]%
\node [coordinate] (inText) {};
\end{tikzpicture}%
%
% Make the margin par
\marginpar{%
\begin{tikzpicture}[remember picture]%
\definecolor{orange}{rgb}{1,0.5,0}
\draw node[draw=black, fill=orange, text width = 3cm] (inNote)
{#1};%
\end{tikzpicture}%
}%
%
\begin{tikzpicture}[remember picture, overlay]%
\draw[draw = orange, thick]
([yshift=-0.2cm] inText)
-| ([xshift=-0.2cm] inNote.west)
-| (inNote.west);
\end{tikzpicture}%
%
}%

\definecolor{light-gray}{gray}{0.95}
% Compensate for fbox sep:
\newcommand\Hi[2][light-gray]{%
  \hspace*{-\fboxsep}%
  \colorbox{#1}{#2}%
  \hspace*{-\fboxsep}%
}

\title{Technical Report}

\begin{document}
\onehalfspacing
\maketitle

\tableofcontents

\todo{Titles are subject too (and probably should in a lot of cases) change}

 
\section{Introduction} 
 
\subsection{Motivation} 
What is the internship; motivation and description 
 
As part of my undergraduate studies I have elected to participate in the internship program run by the School of Computer Science and Statistics in Trinity College Dublin. The main motivation for this is the personal belief that education is made meaningful by application; that the context of a professional working environment would help to cement the knowledge gained in university. It should also draw attention to area of weakness that need to be improved upon. 
Additionally, a professional working environment has a substantially different focus to that of a university. While both depend on the individual to self-educate to achieve a certain goal, the motivation for this, how they go about it, and what goals are set differ substantially. My hope was that an internship would remove the play-pen walls and make me accountable for what I achieved or failed to achieve. University has a focus on collaboration, but in the majority of cases assessment is carried out on and individual basis. When working on a team, an individuals effort directly impacts the work done by others and as such there is greater accountability for ones actions. I have experienced all of this during a previous, shorter internship and seen the benefit in my attitude towards work and study as well as my overall personal development. 
 
Description here 
 
\todo{Rewrite that awful paragraph above}  
 
\subsection{Goals} 
 
Upon arriving at MasterCard I was introduced to some of the team and my mentor, Peter Groarke. We immediately set to work and a brief road map was laid out. No formal goals were decided but Peter and the others walked me through a series of steps: 
\begin{itemize} 
\item Set up a working development environment. 
\item Become familiar with software development life cycle at MasterCard and supporting infrastructure. 
\item Contribute functioning code. 
\end{itemize} 
The main objective was to put me to work as soon as possible. This was my own personal objective and, happily, seemed to be in line with the objective of my mentor. As far as I am aware, there was no overall plan for the entire internship at the beginning. An assessment of my skill level was needed before that could be considered. 
 
The first task assigned to me was to interface SMART with Cronos. SMART, Something MasterCard Authorization Rules Tool \todo{Get acronym} is an in-house system testing tool used to send mock requests to the Retail API server. Cronos is another server used for retrieving certain system information. My own personal goal was to learn new and interesting programming concepts and technologies. \todo{Loads more detail} 
 
\subsection{Approach (State of the art)} 
Slightly reflective, what was my approach to the internship. Set goals with mentor, spent time learning system, learning new skills, development. Repeat steps 2 to 4. In reality is more development and learn as I go. 
 
As stated, my initial goal was to start contributing as quickly as possible. This meant checking working code into subversion. The first step was to setup a working development environment.  
The majority of the first week was spent trying to install and configure a working GNU/Linux environment. The vast majority of the development work done in InControl targets Unix platforms. Linux is also my personal preference of operating system. The InControl department maintains a script used to automatically configure a new development environment. The idea was to simply setup a working Linux install and run the script. The first distribution I tried was one I had used for years. However, the updated version was considerably unstable when I attempted to deploy it. It took three days for me to identify the problem as the distribution, not the in-house environment setup script. I switched distribution and had a working environment within hours. The next step was to become somewhat familiar with the InControl Platform. This consisted of reading any documentation I could get my hands on, experimenting with the system itself and looking through the system database, the core of the InControl platform. 
InControl have a substantial but somewhat poorly indexed wiki which become my main source of documentation, other than the developers. Aside from the InControl platform I also tried to understand how credit card transactions were routed and processed. This was necessary to give the InControl platform context. 
A large portion of my first few weeks was spent learning how  
 
\subsection{Overview of contents} 
 
 
\section{Internship in Context} 
SDL goes here. Derive requirements of testing from literature; black box functionality testing. Derive rest of requirements from actual company requirements. Describe existing system, enough detail to give context. 
 
Orbiscom Ireland, acquired by MasterCard in 2009, develops a product called InControl. The acquisition allowed MasterCard to incorporate InControl into their Value Added Services, a range of products that tie in closely with their core business, the processing of electronic payments. Until 2010 MasterCard Ireland was still primarily concerned with the development of InControl. A division of MasterCard Labs was setup within MasterCard Ireland in 2011. As MasterCard continues to grow, more and more departments are being given a presence within Ireland. This report documents the activities and projects undertaken while working for InControl. Since their acquisition Orbiscom have become completely integrated into MasterCard and have ceased operating as Orbiscom entirely. InControl has been split into two different products. InControl Direct exists to support Orbiscoms original customers as is managed and maintained entirely within the InControl department. The second product, InControl is an adapted version of the original product. Where InControl Direct is deployed to and hosted by banks, InControl is hosted on BankNet, MasterCardâ€™s global payments network. In order to explain the services offered by the InControl platform, it is useful to explain how the credit card payment process works. 
 
InControl and Orbiscom are used somewhat interchangeably in the rest of the document. Orbiscom is a keyword used throughout the code base. All of the packages still begin with \textit{com.orbiscom} 
 
\subsection{The Four Party Model} 
The credit card payment scheme employed by MasterCard involves four separate parties, for this reason it is referred to as the four party model. There are alternate models in use, but MasterCard employs the four party model as it allows the issuing of payment cards to be handled by separate financial institutions. This leaves less overhead for MasterCard and also insures interoperability between the various financial institutions. The credit card holder typically initiates the payment and is represented by a credit card issuing bank, or an issuer. The merchant involved in the payment is represented by an acquiring bank, or an acquirer. Each successful payment goes through three stages; Authorization, Clearing and Settlement. 
\subsubsection{Authorization} The card-holder submits their payment card details to the merchant. The merchants bank, the acquirer, sends a request to MasterCard to identify the card-holders issuing bank. Once this has been determined and the card has been verified the payment is forwarded, by MasterCard, to the issuing bank. It is worth noting that this is the stage in the process where the InControl service is applied if needed. The card-holders bank approves the purchase and blocks the funds in the card-holders account. No money has been transferred from the card-holders account, the money has merely become unusable by the card-holder. The issuing bank forwards the approval to MasterCard, who forwards it to the acquirer who in turn forwards it to the merchant. The payment has now been approved.  
\subsubsection{Clearing} Sometime after the payment has been authorized, usually at the end of the week, the merchant submits all of their authorized payments to clearing. This is a batch process that occurs at certain set times. After a payment has been cleared the funds have effectively been transferred. 
\subsubsection{Settlement} Come back too, concerns the actual transfer of funds. 
 
To facilitate this, a fee is applied to each transfer. Interchange is typically charged by the issuer to the acquirer. This is also where MasterCard makes money from each payment. The rate of interchange varies from bank to bank and can even be different depending on the nature of the purchase. In order to sustain credit card usage and adoption the services offered by MasterCard must outweigh this additional fee. This report will simply assume the following. The interchange fee is to some extent ultimately paid by the merchant. This means that they lose an amount on every purchase made with a payment card as opposed to cash. In order for merchants to continue to accept card payments there must be sufficient consumer pressure for acceptance by the merchants. This is done by incentivising consumers with additional benefits not available through cash payment. Some of these incentives are part of MasterCardâ€™s core network and come as benefits of using an electronic system such as added security, access to credit and accountability. MasterCardâ€™s Value Added Services range is a set of products designed to add additional benefits for consumers. 
\cite{mcInterchange} 
 
\subsection{InControl} 
The InControl platform provides personalized, real-time card controls. It allows consumers to set limits on their credit card and monitor their spending. It is a service provided by MasterCard to credit card issuing banks, henceforth referred to as issuers, who in turn provide the service to their customers. The platform offers a number of services. These are sold as five separate products. 
\begin{itemize} 
\item Cardholder Alerts & Controls: Allows consumers to establish personalized spending profiles on their accounts, and allows for real-time notification of spending activity. 
\item Issuer Portfolio Control: Aims to help prevent cross border fraud in Europe by placing controls on large sets of credit cards. 
\item Small business controller: Allows small business owners to confidently delegate spending to employees. 
\item Purchase control: For large corporations. The service offers unique limited-use account numbers and authorization controls to improve reconciliation for card payments and enhance visibility into spending activity. 
\item Family Solutions: Allows parents to give family members access to credit. 
\end{itemize} 
 
At a more technical level, the InControl platform provides two services; the creation of virtual card payment numbers and the creation of rule sets to be applied to credit card transactions. 
 
The virtual card numbers (hereafter referred to as VCNs) are linked back to the original real card and account and can be used exactly like a normal payment card. At the core of the system is the algorithm used to create VCNs, however similar systems have also existed. The unique feature offered by InControl is that no changes to existing infrastructure are required. If a payment requires InControl to continue processing then the payment is routed to the InControl platform. This happens entirely within the payment network. Neither the merchant nor the acquiring bank has any knowledge that InControl has been applied, so no action is needed on their part. Other similar technologies required the merchant to add knowledge of the process to their point of sale, i.e., they had to replace all of their card readers. 
 
InControl allows an extensive range of controls to be set on payment cards. Rules can be placed on the amount spent, where it is spent, what it can be spent on, along with many others, can all be controlled. These controls can be applied by individuals to single payment cards, or by banks to ranges of issued cards. The ability to control a personâ€™s spending has proved a popular feature. The bulk of InControls traffic is made up of corporate accounts. Many businesses have found that VCNs are an effective way of controlling employee spending.  
 
The original InControl Software platform was designed as a generic server framework to host various payments services such as InControl VCNs as well as other third party technologies such as Verified by Visa. The VCN service was developed by Orbiscom. If a bank chose to provide this service they could use the InControl platform to host other financial services as well. This fucntionaThe entire platform is written in Java and configured using XML. Work began around the same time as the first versions of Java Enterprise were being released. This had a very clear effect on the development of the platform. InControl was designed as a generic server framework because no suitable server framework existed at the time. 
 
MasterCard operates BankNet, a global telecommunications network linking all MasterCard card issuers, acquirers and data processing centres into a single financial network. The operations hub is located in St. Louis, Missouri. BankNet uses the ISO 8583 protocol. The network is peer-to-peer mesh network with a set of endpoints. At no time during my internship did I have any involvement with any aspect of BankNet, but it does have one very notable effect on development within the company, the release cycles. BankNet is the core of MasterCardâ€™s business. One of the most important aspects of an electronic payments service is reliability. If MasterCard is unable to serve its customers in anyway it would mean a huge blow for their brand, a brand they have invested a substantial amount of money in. BankNet must be reliable. This is achieved in part with an extremely conservative attitude towards updating the software that controls the network. The servers can only be restarted twice a year. This takes a considerable amount of effort. Each node must be updated and "flipped" across the entire network. The network cannot be taken down completely during this time and there are many possible issues that are taken into account, hence the twice yearly cycle. There are a further two periods each year where the network can be updated without restarting the core services. All of this means MasterCard uses a quarterly release cycle, with fixed deadlines. Due to the inflexibility of the release cycle, great care is given to selecting new functionality to be included in each quarterâ€™s release. These factors, a rigid deadline, fixed requirements and a very strong reliance of reliability in the end product are traits commonly associated with the waterfall design process, and this is the design process employed by MasterCard. 
 
The waterfall design methodology arguably fits well with the requirements surrounding MasterCardâ€™s core network. Other services, such as InControl are not subject to the same requirements but are none the less hampered \todo{Should probably remove} by the employment of waterfall. During my time at MasterCard a switch an agile design process was beginning to get underway within InControl.\cite{I had a source here but it seems it was lost} 
 
\subsection{Software development life-cycle} 
 
\subsubsection{Terra} 
\label{Terra} 
 
\section{Existing System} 
 
The InControl platform is very complex. It was not developed as a single service but rather as a generic server framework to host various payment services. The platform is component based and hosts a number of generic services and product specific services. The role of InControl has changed over time, with the emphasis now placed the VCN service as MasterCard have no interest in hosting their competitorsâ€™ products. 
 
\subsection{Components Overview} 
The following is a brief overview of the components within the InControl system. At present, the system consists of 21 separate components containing about 768,732 lines of code between them. This is just the core InControl product, there are many other auxiliary projects in the subversion repository. A detailed look at each component is carried out in subsequent sections. 
 
The InControl platform is comprised of seven major types of component. 
\subsubsection{Client applications} 
Client applications include any client interacting with an InControl server. These include the InControl Virtual Card Client and other third part clients, such as Verified by Visa clients. There are two versions of the Virtual Card Application (originally called the O-Card application), the thin and slim client applications both communicate all requests to the VCNs server via the InControl web server environment. 
\subsubsection{Web Server Environment} 
The web server environment acts as a gateway to the InControl Dynamic servers for the external client applications. The InControl Payment platform typically requires that a web-server is available. The web server environment may include web servers or application servers, or a mix of both. The main functions of the web server environment are as follows: 
\begin{itemize} 
\item Provide the content files for InControl client applications, e.g. configuration files, web assets. The client content can be hosted on any plain old web-server. 
\item Host the InControl servlets. The main functions of the servlets are to preform message format conversion to enable InControl servers to process messages from the client applications and the reverse. This is to allow deployment of the InControl Platform into different environments with minimal code impact, a clear benefit of a modular design. Only one component has to be switched out when the platform is moved to a different network \todo{Yeah not really}. An application server providing the appropriate servlet environment is required. The servlets support a number of servers including WebSphere and Tomcat, allowing it to be easily integrated with existing infrastructure. If there is no existing infrastructure it can be run inside the InControl Apollo framework.  
\item Host InControl sessions and authentication manager. The session and authentication manager allows for integration with an existing customer authentication API. \todo{Athena, wrong usage} can also provide servlets for session management and authentication. 
\end{itemize} 
\subsubsection{InControl Servers} 
The InControl servers provide the processing core for the InControl payment platform. A generic server framework is provided for hosting the generic servers, e.g. maintenance and the payment products services, e.g. Virtual Card Number services. The generic InControl server architecture is based on a dynamic server framework. The InControl servers reside on BankNet, while the InControl Direct servers typically reside on a customers, e.g. An Bank, internal network. The services provided by the platform can include Online Services, Batch Services or Scheduled Services. 
\begin{itemize} 
\item Online Services: The platform can be used to provide a number of online services including registration, session and authentication, authorization services and client support services. 
\item Batch Services; Batch services read a file as input and apply changes to the server data based on the contents of each record. They can also be run as command line utilities. The platform provides card settlement, registration and maintenance as batch services. 
\item Scheduled Services: These are programs that can be configured to run periodically in order to preform housekeeping tasks. They can be configured to run inside one of the installed InControl services or to run as standalone processes. 
\end{itemize} 
\subsubsection{InControl Database} At the core of the system is a relational database shared by all components. The system uses an Oracle database because \todo{Find out reasons, don't trigger argument if possible}. The database provides both data storage and configuration information to every component within the system. \todo{Needs further expansion} 
\subsubsection{Customer Service System} The customer service system is web application made available to the clients customer service representatives for dealing with customer service issues in relation to the InControl Platform. The Customer Service System consists of two components. A web server used by the client and a backend server used to query the InControl systems database. 
\subsubsection{APIs} APIs are available to either access functionality of the system, ex. the registration API, or to provide functionality to the system, ex. the user authentication and session management for the platform. 
\subsubsection{Administration Programs} The InControl Platform comes with a number of administration programs that are used to preform or initiate housekeeping tasks. These programs can be run from the command line. Examples of such programs are the archiving process and the Server Status Controller. 
 
\subsection{Web Server Environment} \todo{Citation needed} 
The web server environment provides the link between the external client applications and the InControl Servers, the core of the InControl Platform. Typically web servers and, optionally, application servers are used to provide the appropriate web server environment. 
 
The web server environment typically hosts the Orbiscom servlets. The Session and Authentication Manager can also be hosted within this environment but is typically looked after by the customer in the case of InControl Direct, or by MasterCard in the case of InControl. 
 
\subsubsection{Content Hosting} 
A web server is required to provide the content files for Orbiscom client applications. These files are served dynamically each time the client connects to the URL. Any plain old webserver can be used. 
\subsubsection{InControl Servlets}  
The Web Server / Application Server typically host the InControl servlets. Athena is the servlet system used to support the InControl Platform \cite{Citation needed} 
 
 
 
\subsection{InControl Server} 
This section describes the InControl server architecture, incorporating the generic server framework. The InControl server architecture is described in the following areas: 
\begin{itemize} 
\item Generic Server Framework 
\item InControl Services Overview 
\item Configuration and Installation 
\item Communications Security 
\item Operational Requirements 
\end{itemize} 
 
\subsubsection{Generic Server Framework} 
The Generic Server Framework is based on a dynamic server framework. Apollo and Atlas provide the generic server framework. 
\begin{itemize} 
\item Apollo provides the generic, dynamic, extensible framework for clients and servers. 
\item Atlas provides the InControl HTTP and XML dispatchers. 
\end{itemize} 
These two components are at the core of the InControl platform. Everything is built on top of them. Together they create a full generic server framework with capacity to manage authentication, thread synchronization, component configuration, database connection and connector pooling, security and session management. As they have such an important role, the deserver a detailed analysis. \todo{Insert detailed source level analysis, probably not needed though} 
 
The InControl Servers are UNIX based and implemented entirely in Java. They are designed to be platform independent and will run on any platform that supports an appropriate release of the JVM, at present InControl targets JVM 1.6. 
\todo{Diagram} 
InControl servers can be configured to listen on one or more communications interfaces for incoming connections. Each listener can be configured to process one communications protocol. The platform supports HTTPS, HTTP, raw TCP/IP, MQ Series and X.25. 
 
Each listener can be configured to pass on requests to different types of content handlers. The most common form of content handler is the InControl XML content handler, which process messages that are in a format specified in the InControl Message Document. \cite{GET THAT DOC. Also, I presume this format is now the mastercard format, is it translated into Orbiscom format or what?} 
 
Other message handlers can process various authorization card scheme formats, for example Visa and EuroPay handlers. 
 
InControl Servers can be configured to dynamically update various configuration items upon receipt of a signal from a Platform Administration Utility. 
 
The following is a list describing the behaviour of an InControl server. 
\begin{itemize} 
\item Connection: At boot-up a number of dynamic servers are created to process jobs. Each Server creates a number of connectors. These connector classes allow the server to either listen for requests or connect to external services. The classes, which implement the actual connector protocol, e.g. TCP/IP, are specified in the Dynamic Server configuration. This allows the connectors to be swapped out easily, The connector specification contains the name of the implementing class and a port number. There may also be a number of optional parameters defined depending on the protocol. As the Connectors are implementations of a Connector interface, there are no mandatory parameters. As such, all of the parameters are in fact optional, but omissions will result in exceptions being thrown. These are caught and a default value is returned if specified. This would appear to be a bad way of returning a default value as exception handling is being used for control flow. \todo{Get input on this} 
\begin{verbatim} 
    <Connector 
        Class="com.orbiscom.apollo.net.TCPServerConnectorFactory" 
        Port="12400" 
        TcpNoDelay="true"  
        ReadTimeout="3"/> 
\end{verbatim} 
\item Transport: A protocol handler specifies the Network Transport Protocol that is implemented on the connectors, e.g. HTTP. The protocol handler can specify many content handlers. The handler specification contains the name of the implementing class and a number of handlers. The handlers are indexed by a request type. In the case of HTTP the request type is the path. The handler specification body can be as simple as just the name of the implementing class or much more complex. An example is provided below. The first handler specified is an XML Dispatcher. This content handler converts the incoming HTTP content to an InControl XML request for processing. The other elements of note are the implementing class and the HandlerSet. The handler set contains links to more configuration files. These files contain the mapping from requests to the handlers to service them. Different versions of the InControl protocol may require different handler sets. Great care has been taken to ensure that each new release is backward compatible with the previous version, but if needed separate handlers can be used. 
\todo{Format correctly, verbatim tag just runs off the page} 
\begin{verbatim} 
<ProtocolHandler 
		Class="com.orbiscom.apollo.net.HTTPProtocolHandlerFactory" 
		MonitorHandlerTimes="true"> 
 
		<Handler 
			RequestType="/pulse"> 
 
			<Task 
				Class="com.orbiscom.atlas.xml.Dispatcher" 
				RequireAuthentication="true" 
				LogAuthenticationErrors="true" 
				UseEncryption="true" > 
 
				<DocumentBuilderFactory> 
					<Property Name="NamespaceAware" Value="true" /> 
				</DocumentBuilderFactory> 
 
				<OilContext 
					MessageTimeZone="UTC"> 
				</OilContext> 
 
				<MessageContext> 
					<Variable Name="DataExtract.Source" Value="RetailXMLAPI" /> 
				</MessageContext> 
 
			<HandlerSet> 
				<VersionedHandlerSet Version="12.4" xlink:href = "metahandlers" /> 
				<VersionedHandlerSet Version="12.4" xlink:href = "pulsehandlers" /> 
				<VersionedHandlerSet Version="12.3" xlink:href = "pulse12q3handlers" /> 
				<VersionedHandlerSet Version="12.4" xlink:href = "flexhandlers" /> 
			    <VersionedHandlerSet Version="12.4" xlink:href = "ipchandlers" /> 
				<VersionedHandlerSet Version="12.4" xlink:href = "evcnhandlers" /> 
                <VersionedHandlerSet Version="12.4" xlink:href = "sbchandlers" /> 
            </HandlerSet> 
 
        </Task> 
    </Handler> 
 
    <Handler 
        RequestType="/schema"> 
        <Task 
            Class="com.orbiscom.atlas.util.SchemaServlet"> 
        </Task> 
    </Handler> 
 
</ProtocolHandler> 
\end{verbatim} 
\label{handlerset} 
An entry in a handler set follows. The example provided is a particularly complicated specification to illustrate the widest possible range of functionality. A simple hander consists of a request type name and handler class. In the example entry a constraint and various pre-processors are specified. The constraint classes are built into the server framework and check certain conditions are met before further processing is done. In this case, the supplied PAN (Personal account number or the number embossed on a credit card) is checked to make sure it is the correct length. The pre-processor takes the supplied PAN and maps it to a unique ID, which is then used to index the account in subsistent operations. Each pre-processor falls through to the next so the order is important. 
 
\todo{formatting again} 
\begin{verbatim} 
 
    <Handler 
        RequestType="GetVCNListRequest"> 
        <Task 
            Class="com.orbiscom.pulse.oil.GetVCNListXMLHandler"> 
 
            <Constraints> 
                <Attribute Name="Pan" Constraint="PanConstraint" /> 
            </Constraints> 
 
            <OilProcessors> 
                <OilProcessor Class="com.orbiscom.pulse.oil.mapper.PanMapperProcessor" /> 
                <OilProcessor Class="com.orbiscom.pulse.oil.obo.CheckThirdPartyAccessProcessor" /> 
                <OilProcessor Class="com.orbiscom.pulse.oil.GetVCNListHandler" /> 
            </OilProcessors> 
        </Task> 
 
        <Task 
            Class="com.orbiscom.atlas.mastercard.audit.AuditLogger" 
            EventType="com.mastercard.common.jal.events.EventType.ACCESS_CARDHOLDER_DATA" 
            AuditMessage="Get VCN List" > 
        </Task> 
         
        <ErrorTask 
            Class="com.orbiscom.atlas.mastercard.audit.AuditLogger" 
            EventType="com.mastercard.common.jal.events.EventType.ACCESS_CARDHOLDER_DATA" 
            AuditMessage="A request to Get VCN List failed" > 
        </ErrorTask> 
    </Handler> 
 
\end{verbatim} 
\item Application: The XML Dispatcher determines the issuer \todo{Clarify the term Issuer at the start} the request is intended for and starts a transaction in the database configured for that issuer. The XML Dispatcher iterates through all the elements in the request message. For each node, the dispatcher determines the relevant application handler and runs it. The handlers are run in the order specified by the request. The response XML document is created and each application handler appends its response to this document. 
The platform is state-full. The dispatcher maintains the database transactions per request by creating a massage context. This context object is passed to each application handler in turn. The dispatcher only commits the changes when every handler working on the request returns successfully. 
If an exception occurs while iterating through the nodes the dispatcher roles back the database transactions. Further processing stops and failure message is returned. The response will specify a return code and a message to indicate the nature of the failures. At present, the core system, e.g. Atlas and Apollo, have X \todo{get that} individual error codes defined. Each component may also define its own failure codes, for example, the Retail API Server has Y \todo{get that}. These codes are of the format NNXXX, where NN is the component name and XXX the error code. The code is always followed by a textual description of the failure. ex, AT001 General Error. Here AT is the component name, Atlas and 001 is the error code. Often it is fairly trivial to quickly trace the cause of an error because of this. However, error codes are generally defined for well understood and anticipated error. /todo{Get confirmation on this, maybe expand?}. 
An additional task is also run in this instance. The event is written to an audit file for logging purposes. The audit file is a separate database supplied by MasterCard called the data-warehouse. The idea is to offload logging from the main system. 
After the request has been processed the XML dispatcher completes the audit message by writing a trailer recorded. This record indicates whether or not the transaction was successful. 
I was curious about the reasoning behind state-full request processing. Once again this can be attributed to the difference between taught best practices and real world scale. The InControl platform, as evidenced, is not trivial. Many of its operations and use cases are complex and it makes sense to preserve state in many instances. This keeps the size of any one operation to a manageable size and allows reuse. While changes in a specific request may (and have) had a negative effect on other requests it is still a much better approach than trying to make every request stateless, and thus huge. \todo{More of this questioning and "critical thought", Also make sure if have drawn attention to things being different in the enterprise by now in the text. Also that last paragraph is particularly bad} 
\item Client Mode: These are servers than can be configured to initiate connection to another server rather than listen on a port. The poll this connection for requests. They can use same protocol and same content handlers as any other InControl Server. An example would be the InControl Registration server when communicating over MQ Series. 
\end{itemize} 
\subsubsection{Configuration} 
All communication from applications external to the server is configurable. InControl platform components are highly configurable and the configuration files have grown substantially, accounting for just over 7\% of the code base. Every component has its own set of configuration files. There is also a set of common configuration files \todo{Expand?}. The system consists of five general types of configuration file. Each file is used to configure a different aspect of the system. Any file may link to several other files, but the following can be viewed as the five root configuration files. 
\begin{itemize} 
\item Top Level configuration: This is the first configuration file read on start-up. It is passed as a command line argument and is responsible form specifying the various configuration items the server has access to. The following configuration files are contained as nodes. 
\item Logging: The log4j configuration file contains the logging configuration for the system. Log4j is a third party component used by the InControl platform. It employees configurable logging levels for different aspect of the system. The current debug level can be specified in the file. During development full logging will be enabled, but in a production environment INFO or ERROR level are used. This ensures only relevant information is recorded. It also keeps writes to log files down. IO operations are generally very costly, so being able to easily change the logging level is very beneficial.  Additionally, a typical log4j configuration file will contain a number of loggers. These are specific to classes or packages in the classpath of the running Java application. Loggers contain a logging level at a minimum. 
\begin{verbatim} 
<logger name = "com.orbiscom.atlas.range"> 
    <level value = "info"/> 
</logger> 
\end{verbatim} 
Appenders are also specified. Crucially, these elements define where a log is written and the implementing class, as well as information such as the character encoding used and time-stamp patterns. 
\begin{verbatim} 
<appender name = "Debug" class = "org.apache.log4j.DailyRollingFileAppender"> 
    <param name = "File" value = "log/debug.log"/> 
    <param name = "Append" value = "false"/> 
    <param name = "Encoding" value = "UTF-8" /> 
    <layout class = "org.apache.log4j.PatternLayout"> 
        <param name = "ConversionPattern" value = "%d{dd,MM HH:mm:ss:SSS} [%t] %-5p %c{2} - %m\n"/> 
    </layout> 
</appender> 
\end{verbatim} 
\item Database configuration: The database configuration specifies all the databases that an InControl Server accesses. It contains the database URL, the username, password and the database driver class. This information resides in a ConnectionPool node.  
\item Dynamic Server configuration: The Dynamic Server configuration section defines the operation of the server, the actions it will perform, supported message types and the supported protocols. The ProtocalHandler element shown above is from a dynamic server configuration file. It also contains links to the handlers used by the servers, again illustrated above. 
\item Classloader configuration: \todo{Relevant? If not, change 5 to 4 above} 
\end{itemize} 
\subsubsection{Communications security} 
Inter communications between the components and also between a component and an external server may be secured. SSL can be used, in such cases certificates must also be configured. Signed data can also be used, in which all communication is signed with a pre-shared key. The Java Cryptography Extension framework is used to preform data signing. JCE was used as it is standard cryptography framework for the Java SDK. The system signs the data with hash-based message authentication codes using SHA-1. The keys are contained within jar files as this is the easiest and safest way to store them. 
 
\subsection{Session \& Authentication Service} 
The Session \& authentication Service is responsible for verifying the cardholdersâ€™ credentials, thereby ensuring that they are valid users of the InControl system. 
A flexible authentication service is provided, where Issuers have the option of implementing any of the following authentication sessions. 
 
\subsubsection{InControl Internal Authentication} 
InControl is capable of managing complete authentication. The user credentials are stored in the inside the InControl database. SHA-1 is used to encrypt the passwords. 
Verification rules for User credentials can be configured in the database. These rules are validated when creating the user credentials, e.g. Minimum number of numeric characters required, max invalid logins etc. The number of possible rules has grown considerably, from three or four to over thirty five. These values for these requirements have stayed fairly static for a number of years. A default set are usually set for each new issuer, and changes made thereafter if required.  
 
\subsubsection{External Authentication} 
The InControl Session \& Authentication Manager (SAM) may be hosted in the web server environment or on the InControl Server platform. The SAM acts as a wrapper for an issuers' authentication and sessions APIs. InControl servlets authenticate VCN requests through the SMA API rather than linking directly through the issuers API. This is again part of the modular design employed throughout the project. The various servlets do not require customization for each customer, only the customer specific SAM. This modularity is only made use of in InControl Direct. After the InControl platform had been updated to operate with BankNet, session and authentication were no longer an issue as BankNet takes care of them. 
The SAM classes are specified in a servers properties file. 
 
\subsubsection{Session Management} 
To avoid the requirement for the client to send their complete user credentials in every message the InControl platform supports session management. When a clientâ€™s credentials are successfully validated a unique 40 digit session ID is generated using a cryptographically secure hash algorithm and sent to the client. This token and the unique user ID are stored in the InControl database in a temporary table. On future requests the client need only send their token and the platform will accept the message. The timeout for a session can be configured in the database. 
 
\subsection{Client Support Services} 
\todo{later} 
 
\subsection{Online Registration \& Maintenance Service} 
The Registration and Maintenance of cardholder and card information in the InControl database is required to allow cardholders to avail of the payment services provided by the InControl Platform. This service is provided by the Retail API Server and the implementing component is Pulse. The example config files above are taken from the Retail API Server. 
 
\subsubsection{Functionality} 
The Retail API Server receives and processes XML formatted requests. The server listens for incoming valid XML requests from an external system on a configurable port. A defined attribute in the XML request will determine the action to be taken. The service can also supply default values for any non-specified data. These values are specified in the implementation and cannot be configured. 
Once the request has been processed successfully, a response will be sent back to the source informing them of the success or failure of the request. 
The Retail API server encompasses the majority of the InControl Platform functionality and is used by several other components, in addition to being one of the customer hooks into the InControl Platform. 
\subsubsection{Message Formats} 
All messages sent to and processed by the Retail API Service are XML documents transported by HTTP requests. The HTTP requests are fairly ordinary, containing information such as host, port, service etc. 
 
The HTTP message body contains the information to be processed by the Retail API Server. The root node is known as the requester. A requester can contain N individual requests. If any one request fails, the overall requester fails. The response is structured similarly, the response root node contains the response value, e.g. Success or Failure, and N response elements containing the response data specific to each request. 
 
An example XML request is shown, populated with mock data. 
\begin{verbatim} 
[<?xml version="1.0" encoding="UTF-8"?> 
<OrbiscomRequest IssuerId="1" Version="12.4"> 
  <AddRcnBinRangeRequest IssuerId="99993"> 
    <Range CardType="83" CpnType="DF" EndRange="1234569999999999" LanguageId="1" Prefix="123456" StartRange="1234560000000000" Status="A"/> 
  </AddRcnBinRangeRequest> 
  <SetBinRangePropertyRequest IssuerId="99993"> 
    <Property Name="ICA" Value="009661"/> 
  </SetBinRangePropertyRequest> 
  <SetBinRangePropertyRequest IssuerId="99993"> 
    <Property Name="AICABRC" Value="5"/> 
  </SetBinRangePropertyRequest> 
  <SetBinRangePropertyRequest IssuerId="99993"> 
    <Property Name="IPCAMSRequired" Value="0"/> 
  </SetBinRangePropertyRequest> 
  <SetBinRangePropertyRequest IssuerId="99993"> 
    <Property Name="DefaultIPCProfile" Value="DEFAULT"/> 
  </SetBinRangePropertyRequest> 
  <SetBinRangePropertyRequest IssuerId="99993"> 
    <Property Name="IPCPorductType" Value="FC"/> 
  </SetBinRangePropertyRequest> 
</OrbiscomRequest> 
\end{verbatim} 
 
 
\subsection{Batch Registration and Maintenance Service} 
 
\subsection{Rules \& Controls} 
 
\subsection{Virtual Card Generation} 
 
\subsection{Virtual Card Settlement} 
 
\subsection{Virtual Card Authorization} 
 
 
\cite{Orbiscom Payments Platform Technical Architecture} 
\cite{http://wiki.orbiscom.com/index.phpPlatform_Documentation} 
\cite{Cite more things so it doesn't looked as much like pasta, and spread them thought the text} 
\cite{Also cite source code all over the place} 
\cite{Stored procedure for dummys} 
 
\section{SMART} 
Objectives. Describe SMART. Describe what was done (BUGS!). How were bugs tested / found?. Don't forget the impacted work on Cronos / Pulse. OIL goes here I guess. 
 
\subsection{Overview} 
 
Something MasterCard Authorization Rules Tool \todo{Get acronym} is an in-house platform testing tool used to send mock data to the Retail API Server. The bulk of platform testing preformed within InControl is done manually. SMART is used to somewhat simplify this process by providing a GUI application to generate mock requests for a wide range of possible use cases. It began as a small application to generate account level services / spend \todo{which one?} rule sets and submit them to the Retail API Server. Over time it grew into a testing platform in its own right, capable of exploiting the vast majority of functionality offered by the Retail API Server and thus the InControl Platform. SMART provides an interface into a reasonable slice of InControl platform functionality. The InControl platforms main function is to provide virtual card numbers and enforce rules associated with them. SMART allows for the creation of virtual and real card numbers, enabling InControl services on those cards and assigning various controls and services. 
 
\todo{Some screen shots} 
 
 
\subsection{Objectives} 
SMART is used by InControl developers, the InControl Quality Assurance department and several other offshore departments involved in the development and testing of InControl. The vast majority of work performed during the internship involved the development and maintenance of SMART. The work done can be split into two categories, integration of SMART with Cronos and maintenance. 
SMART was my first task, so the initial goals were reasonably broad and changed as the work progressed. The first task, to integrate SMART with Cronos had two objectives. First, to produce some sort of meaningful contribution, and second to allow evaluation so future tasks could be planned with respect to my abilities. After this task was completed I was given official ownership of SMART and tasked with maintaining and updating it.  
 
 
\subsubsection{Cronos} 
Cronos is a dynamic InControl server. It provides functionality that is not exposed to an end user or issuer. Its purpose is to help automate some of the steps needed when adding a new issuer into the InControl system. Adding a new issuer is not a trivial process. There are several significant database inserts required. These steps differ depending on the product the issuer intends to use. A large amount of the set up data is static and thus a large amount of the set up process can be automated. Cronos can also be used to retrieve certain information from the system. \todo{More background?} 
I was tasked with extending SMART to allow easy interaction with Cronos. There was no existing way of communicating with Cronos other than sending raw XML requests. The desire was to have SMART provide a GUI interface to create these requests and view the responses. 
In order to accomplish this I had to learn how to interface with the existing InControl components. The amount of time taken to do this was vastly underestimated as I had no comprehension of the code complexity of a commercially developed software platform. To complete the work successfully I broke the task into several subtasks; \todo{Should this goes in the approach? Probably} 
\begin{itemize} 
\item Download, configure and use SMART. \todo{Cocked up with build system, manually got jars} 
\item Become familiar with the structure of code SMART. 
\item Get basic communication with Cronos working. 
\item Create an interface for Cronos in SMART. 
\item Add support for all of the Cronos requests into SMART. 
\end{itemize} 
The main problem encountered while trying to initially use SMART was configuration. As previously described \todo{Previously describe it!}, most developers have slightly different ways of doing things. I was unaware of a running instance of the Retail API server because the developer I asked never worked on it, and their work flow usually involved running any needed components locally. An instance of the Retail API Server was setup on my local machine. Terra automated this process to a large extent but some configuration information still had to be specified manually. Once this was done some time was spent using SMART.  
 
\subsubsection{Maintenance} 
As SMART is a testing tool I began to research software testing and read some of the surrounding literature. I quickly realized that while there is a lot of information and discussion about how to test software, what to look for and the strengths of a particular method over another, there is very little on how to implement the tests themselves. The Standard for Software Component Testing, produced by the British Computer Society does not describe how to achieve the required attributes of any test process \cite{BCStesting}. This seemed odd at first, but after further reading I understood such a description is impossible. The essence of a software entity is a construct of interlocking concepts. The difficulty in software development is the specification, design, and testing of this conceptual construct, not the construction or its verification. \cite{NoBullet} Every piece of software has a different set of requirements and thus a different design. There is no true standard for developing software so there is no true standard for testing it, just a collection of additional concepts that may be employed.  
\begin{quote} 
In a broader view, we may start to question the utmost purpose of testing. Why do we need more effective testing methods anyway, since finding defects and removing them does not necessarily lead to better quality. An analogy of the problem is like the car manufacturing process. In the craftsmanship epoch, we make cars and hack away the problems and defects. But such methods were washed away by the tide of pipelined manufacturing and good quality engineering process, which makes the car defect-free in the manufacturing phase. This indicates that engineering the design process (such as clean-room software engineering) to make the product have fewer defects may be more effective than engineering the testing process. Testing is used solely for quality monitoring and management, or, "design for testability". This is the leap for software from craftsmanship to engineering. 
\end{quote} \cite{cMellonTest} 
 
The testing done within InControl is done to ensure platform reliability. This is carried out through black box testing using techniques such as Equivalence Partitioning and Boundary Value Analysis. As the specification of the inputs used in these techniques is essentially arbitrary, SMART must allow for arbitrary input. In practice, SMART is able to construct structurally correct InControl XML requests containing arbitrary data elements. SMART does preform some checks before sending the requests. I decided to leave practical checks, such as not allowing the user to input a string when a number is expected, but removed any controls I saw as blocking possible test cases. The argument can be made that any checking done by a testing tool is blocking a valid test, but there are practical considerations to make. SMART has a small group of user, people who spend many hours per week using it. Some checks done within SMART exist to make the users life easier. Furthermore, some of the basic checks are simply out of scope for testing. It is well understood that entering a string where a number is expected may happen, and there are well established mechanisms already implemented to handle such cases. Conversely, there are scenarios that need to be tested. A card might be registered on the system with an expiry before the date of registration. The expected behaviour here is ambiguous and depends on the design of the system, so it makes for a valid test case. This was the balance I tried to achieve while working on SMART.  
 
\todo{REFLECT ON THIS} 
 
\subsection{Description} 
 
SMART began as a very small Java GUI application to allow controls to be placed on a card for testing. It has since grown into quite a substantial testing tool, consisting of over 30,000 lines of code. Its high level function is very simple, to allow a user to easily create valid XML messages and send them to the Retail API server. It has grown to be quite complex because of the varied nature of the requests it constructs. The UI contains most of the code, about 25,000. It could well be argued there are many opportunists in the design to shrink this number down, as there is a lot of duplication, but it gives a clear indication on where the focus lies. 
\label{OILIntro} 
The code follows an MVC type of model. This is reflected in the projects package structure \todo{Diagram of package structure}.  
The model is supplied by external libraries. These libraries contain the request objects and their supporting data objects. All of these libraries are generated by the OIL compiler (p. \pageref{OIL}). The OIL specifications for these libraries are contained within their respective components, e.g. Pulse is the library used to communicate with the Retail API Server and the OIL definition file is part of the Pulse project. The Pulse library is marked as a SMART dependency within the build system, Terra (p.\pageref{Terra}). OIL is an interface definition language. The model, in its entirety, is generated automatically. This automated process, a small simple specification transformed into many thousand source lines of code, helps to eliminate errors from the model almost entirely. Model objects are generally quite simplistic in nature but are often large, particularly when the model is considered as a whole. If they were written manually there would be many opportunities for error. The use of an IDL language also ensures consistency of the model design throughout the platform \cite{IDL} 
The controller, as expected contains the logic to translate the request and responses issued to and from the server into objects that can be represented to the user. This is also where it begins to become pretty clear that SMARTs design does not conform to an MVC structure very strictly. The main purpose of an MVC design is to decouple data from its presentation. The data, in the form of the model is represented by the view. When a model changes, the view can be updated to reflect this change. Representing data as abstract objects allows for greater decoupling. The underlying structures of a model or view can change completely so long as the presented behaviour does not change. SMART does not pass model objects from the views to the controllers. The view passes a number of simple data types to the controller which are assembled into a model object and sent to the server. The result is a massive amount of code duplication. One method is used to send the request, but each separate request has its own separate method used to construct it. Similarly, each request has its own distinct view. This was something I wanted to avoid doing. \cite{GOF} 
When I to work on SMART I created sub-packages within the MVC structure to contain my work. This was to make the difference in functionality absolutely clear and also so I could learn more about interacting with the InControl platform by doing so from scratch while using the existing code as a base. 
Because SMART is used heavily by the QA team to interact with the Retail API server a lot of bugs logged against SMART are in fact problems with the Retail API server. The vast majority of these bugs could be traced to differences in environments or uncompleted code being checked into the repositories. I did however find some small bugs in the Retail API server misclassified as SMART issues. 
 
 
\todo{Explain code structure + diagram. MVC etc.} 
\cite{Something on MVC} 
 
\todo{Detail exact functions, donâ€™t forget sending raw XML to other components} 
 
 
 
  
\subsection{Cronos} 
After reading some of SMARTs code, testing its functionality and reading relevant InControl documentation work began the new features. SMART had only interacted with the Retail API server. It was to be extended to interact with Cronos, another server within the InControl platform. 
 
\subsubsection{Initial work} 
The first goal set out was to get SMART to communicate with Cronos by sending a simple request. Cronos sub-packages in the API and View packages where created. A simple request object was created and sent in a requester object to the Cronos server and the response printed to the terminal. These first few steps took quite a while. There was some documentation but this initial work was based mainly off the existing code in SMART. The existing code in SMART seemed needlessly complicated and it was quickly assumed there was a better way of achieving the same result. The existing code in SMART for sending a request executes as follows: 
\begin{itemize} 
\item Each individual request has a unique trigger somewhere in the view, usually in the form of a button. 
\item This trigger is hard coded to a call in the API class. A number of simple data types are passed from the view to the controller (API) via this call. 
\item This initial call within the controller constructs a model object representing the request and passes it to send method. 
\item The send method decomposes the request(s) into their XML representation. A HTTP request is then constructed and sent to the server. 
\item A HTTP response object is returned by the server and the XML response object is extracted. 
\item The response is passed back up and a model object is constructed from the XML. 
\item The model objects populated by the response data are sent back to the view where the relevant data is displayed. In many instances, a single simple type is passed back up to the view instead. 
\end{itemize} 
I was unaware of the hierarchal structure of the requests at first but immediately questioned the need to manually deconstruct the model to XML. After talking one of the original system architects a far simpler approach was devised. 
\subsubsection{Generic approach} 
\begin{itemize} 
\item Trigger each request from the view. The trigger should populate a model to send. \label{cronos_generic_api} 
\item This model is passed into the API and sent to the server. Each request to be sent has a distinct representation, all of which extend a request interface. These request objects are contained within a requester object, which can also be used to send the requests. 
\item The request responses, which all implement the response interface, are passed back to the view where they are cast the relevant response and displayed appropriately. 
\end{itemize} 
This method when implemented saved a large amount of time. The controller was significantly smaller than the existing one used to communicate with the Retail API server, about 200 lines of code compared to 4000. It also required far fewer supporting objects. While the Cronos controller may not have had some of the more advanced features featured in the Retail API controller, the vast majority of the code cut out was strictly boiler plate. 
\subsubsection{Dynamic approach} 
There was still the issue of having to manually create the request object in the view and populate it with data supplied by the user. This issue is a more complex one. The manner in which the user interacts with the data is quite important. However, as the initial task was to simply display lists of information it was decided that a table could be used to represent display the data. It was decided, although not know at the time, that there must be a way of doing this generically. The idea that every requests object must be created and populated manually was not one worth perusing because of the sheer amount of time it would take. There was also the problem that very little could be learned by writing large amounts of boiler plate code. The option to simply generate the code was also considered but such an approached would not solve the problem as the end result would still be a large amount of boiler plate code. Creating the requests dynamically proved to be difficult. I was not aware of any method that would allow be to create objects dynamically, i.e. to create an instance of an object and populate it without any knowledge of the object at compile time. The functionality desired was outlined and a solution was simple presumed to be possible.  
After the work was done several developers pointed out that this was a somewhat unusual way to approach a problem. In order to properly design a piece of software some idea of how to implement it is required. The approach I took was one I see being encouraged during my time at university. The mentality of experimentation is actively encouraged. Admittedly, during course-work assignments there is always significantly more background information supplied. \todo{Reflect, may also be far too gushy} 
The desired functionality was as follows: 
\begin{itemize} 
\item Populate a list of requests at runtime and allow the user to select one. An interface should be created to allow the user to supply any needed attributes. 
\item Construct and send the requests using the method described above (p. \pageref{cronos_generic_api}) 
\item Display the response(s) in a list, giving each member a meaningful name. 
\end{itemize} 
This body of work was completed over several weeks. A few different approaches were tried before the final approach was decided upon and implemented. 
A requests are specified in and XML configuration file, along with a name to display to the user. SMARTs toplevel \todo{pageref this} configuration file has a link to this configuration file. Using this link, the request specification is read at runtime and stored as a configuration object. A configuration file object simplifies the process of reading an XML configuration file and is used throughout the InControl Platform. It read in the configuration file and allows its contents to be accessed using a simple XPATH notation. The requests are also grouped by functionality, such as requests to retrieve, insert and update information. 
Once the request list is read in a list of the request objects is created. This took a long time to figure out how to implement correctly and many, many approaches were tried. Java supports runtime creation of objects in a number of ways, ranging from the more complex to the arrived solution which is relatively simplistic. \cite{forName} 
\begin{verbatim} 
Request request = (Request) Class.forName(requestClass).newInstance(); 
\end{verbatim} 
\todo{Reflect on why this took so dammed long} 
These requests are then stored inside one of several hash-maps, one for each type of requests, i.e. get, set, update, indexed by their human readable name. 
The requests now in memory were useless if their members could not be accessed. The first method used to achieve this was reflection. Reflection allows a Java program to introspect upon itself at runtime. A list of method names can be extracted from an object and called. All of this is done at runtime, with no compile time knowledge of the objects structure needed.\cite{Reflection} 
This approach worked but the code produced was obtuse. Java is a statically typed language and was not designed to support this kind of behaviour. After querying some developers and some reading it was decided that reflection should be used sparingly. Different languages have different strengths and encourage development in certain ways. Departure from these conventions, while at times necessary, can produce code that is very difficult to read. Maintenance is widely considered to be the largest cost in software development \todo{CITATION NEEDED}. If a piece of code is quick to develop but very hard to understand then it is often regarded as a bad piece of code. \todo{There is not citation, coding is based off heuristics etc.} 
For this reason it was decided a different solution to the problem was needed. Thankfully the InControl framework, specifically OIL, was designed with this exact scenario in mind. As stated (p. \pageref{OILIntro}) OIL generated all of the requests objects and their associated data types. The resulting objects are substantially more complicated than a collection of simple data types and their accessors. An OilBean, an abstract class extended by every OIL generated object, contains a hash-map of member objects. These members can be accessed via unique accessors or directly via the hash-map. This completely eliminated the need for reflection. The OIL generated classes could be introspected at runtime simply by accessing this hash-map. The map also contained metadata for each member. \label{metadata} The metadata object contains information such as member type, any constraints, the container (i.e. If the member is a list of attributes) as well as several other bits of information. This allowed the OIL objects to accessed in a static manner, and resulted in code that was substantially easier to understand and write. 
The hash-map of members was also used to generate the user interface at runtime. The requests were partitioned by functionality so separate interfaces could be used to display them. Get, (sql select) requests return large amounts of information. There were two main considerations taken into account when displaying this information. The user should be able to view all of the information quickly, and have the ability to define filters to find the information they want. A table was used to allow the user to view the information quickly. The table is generated dynamically from the response data. Certain request responses contain values that can be used to index further requests. Each request can have a number of related requests specified in its configuration.  
 
\begin{verbatim} 
<Request Name="Bin Ranges" Class="GetBinRangeList" > 
    <Related Name="Vcn Bin Range"/> 
</Request> 
\end{verbatim} 
 
This manifests to the user in the form of a right click menu. If a request has one or more related requests, the user can right click on any of the table items and choose from the related requests. The members of the selected row of response data are matched against the parameters to the selected requests. The matching is done by member name, i.e. If a response contains the member "CardId" it can be used to index a GetCardById request. When a match is found, the request is sent using the values from the selected row. This allows a user to quickly drill down through the data. Prior knowledge of what requests are related is needed when writing the configuration file. No checking is done at run time. If a request is specified as related and the parents response does not contain any related members and error is thrown. 
 
\image{smart_get_requests.png}{Table view of request response data and related requests.} 
 
 
Response data can also be filtered. Custom filters can be defined on any column in the table. The user specifies a string to pattern match on. A basic match, analogous to an sql like filter, is then performed on all of the entries in that column. Any number of filters can be applied to any number of columns. These filters are then joined using an "and" operation, allowing the user to precisely locate a piece of information. Again, all of this is done at runtime based on the response data. 
 
\image{filters.png}{Filter creation.} 
 
Add and set requests (sql insert and update respectively) require a different user interface. The primary concern here is allowing the user to enter a number of parameters quickly. The parameters for a given request are used to populate a panel. This is done in a relatively simple manner. A grid is created and the attributes are inserted into the grid. An attribute consists of a label for the name and a component used to input data. The input components change based on the value of the attribute. The possibilities are a text-field restricted to a string or a number, or a combo-box for binary attributes. OIL allows default data to be specified when defining a request. This default data is hard-coded into the generated objects. If default data is present, the relevant UI elements are created with this default data already inserted. This saves the user from having to re-enter the data, but it can still be changed if desired. 
A slight concession was made when handling binary attributes. OIL does not have a binary type, only string and number. Binary attributes are simply strings that can only be "Y" or "N". If an attribute had a "Y" and "N" as default data, it was presumed to be a binary attribute and displayed as a combo-box.  
Strict correctness is at times compromised to create a slightly better user experience. Automatically generating the user interface resulted in substantially less code, about 2,000 lines compared to 20,000 lines. However, unlike the API classes where most of the functionality was retained, the existing user interface contains a lot of unique functionality. The auto generated UI sacrifices much of this functionality in favour of a more generic approach. While this did save a lot of time writing the API, such an approach is harder to justify for a UI. Coding each UI individually, while liable to creating a large amount of redundant code as in SMART, leads to greater level of customization overall.  
 
 
\image{smart_generated_ui.png}{UI for an AddIssuerScheme request} 
 
 
 
\todo{Reflect on using oil the way it was intended without instruction} 
 
\subsection{Maintenance} 
 
After the work integrating Cronos was completed focus shifted to maintaining and updating SMART. SMART is used heavily by members of the Quality Assurance team within InControl, as well as several off-site teams. Due to this relatively intense usage bug reports are fairly frequent, some two hundred and twenty plus since work on SMART began in late 2010, the vast majority of which are closed.  
There are a few important factors that contribute to the amount of bugs in SMART. The primary reason is shifting requirements. SMART began as a very small application to allow the creation of card controls. Over time it has evolved to leverage the majority of the functionality provided by the Retail API server. With each new release of the Retail API server comes new features. These updates must be reflected in SMART. Sometimes these new features are implemented in SMART after they are implemented in the Retail API, when QA is ready to begin testing. This was not always the case, but there were many times when I was made aware of new features within the Retail API because they broke some of SMARTS functionality. 
Another contributing factor is the nature of SMARTs usage. It is QAs main "window" into the Retail API server \todo{Too much?}, so many bugs are miss-classified as SMART issues when they are in fact problems within the Retail API server. 
The final factor is the nature of QAs test cases. A certain chunk of Retail API functionality may or may not be tested to the same degree in each release. Typically, bugs in SMART are found when a certain piece of functionality is exercised extensively in-line with a particular test-case. 
A brief summary of the work done is included in the appendices \todo{SVN LOG} 
 
\subsubsection{New functionality} 
After it was established I had the capacity to maintain SMART I was tasked with updating it to allow testing of new features in the Retail API to be released in quarter three. This was done on a very tight schedule, the first release was to be made to QA three days after the task had been assigned. This release was important to InControl. It had been decided to use InControl to combat fraud, within the German market at first. This was one if the biggest releases ever for InControl in terms of new issuers being on-boarded. The aim was to utilize the existing InControl rules, along with its well developed support services to cut down on cross boarder fraud. The release window was tight in order to compete with similar a service being offered by another large credit card company. Although SMART is by no means core to an InControl release, it does help speed up testing and ultimately the release cycle. There were a number of items that had to be completed in SMART; 
\todo{Make sure ALS and IPC rules are explained, pageref them} 
\begin{itemize} 
\item The addition of a new product, "Fraud Control", to be enabled on a card. 
\item The addition of a velocity control within the "Issuer Portfolio Controls". 
\item Retrieval of rules associated with a card based of an ID. 
\item The addition of a new Region control within both the IPC and ALS controls. 
\item The addition of a new Multi De control with the IPC controls 
\end{itemize} 
These tasks varied substantially in complexity and together serve as a good illustration in the variance of work carried out on SMART. This was also the first time the existing SMART code had to be modified since my work began on it. Up until this point the work done was in relative isolation to the rest of SMART. Not only did these changes have to be implemented, the structure of SMART and how it operated had to be understood in some detail. Additionally, these changes were not yet completed within the Retail API, so once a feature was implemented there was not always a way to verify correct operation. 
 
The first task was a simple addition to the toplevel configuration file. 
 
A velocity control allows a spend limit to be applied to a card, either cumulative over a time period or a maximum allowed spend. This control was already implemented in the ALS controls section, so it was merely a matter of finding it and copying it across to the IPC section and changing the request sent out. In practice this functionality was not as trivial to copy as both the UI and the API needed several changes. This change was not tested as the corresponding call had yet been implemented in the Retail API server. This was my first exposure to seat of your pants coding. \todo{That will probably have to come out} 
\image{ipc_velocity.png}{IPC Velocity control user interface.} 
 
IPC rules are applied to ranges of credit cards, specifically the bin range \todo{page ref}. An IPC rule set is assigned a name. This name is used to retrieve rules from the database for a range of cards. The new release mandated that the card number could be used to retrieve the associated IPC rule set. SMART was modified so they key used to retrieve the rules, the profile name or card number, could be specified by the user. The appropriate request was then called. No modification in how the rules were displayed was needed, save for not allowing rules to be updated when retrieved with a card number. These changes were relatively minor but still required a moderate amount of modification to the code. 
 
InControl has a control to limit card usage based on country. It was decided to group countries into regions and add it as a control. The existing Country control code was used. All that needed to be changed was the data assigned into the control, a list of regions as opposed to a list of countries, and the underlying request sent to the server. This control was to be added to both IPC and ALS rules. Due to SMARTs code structure, this meant a large amount of code had to be directly copied. This effectively duplicated the amount of time needed to test the change. 
 
The addition of the Multi De control was done in a similar way to the Region control. There was a pre-existing De control \todo{What is a DE control}. Happily it had been implemented so the user could create multiple De controls within the one interface. These were then assigned to the rule set as distinct controls. All that had to be done was take these distinct controls and assign their values into one Multi De control. 
 
These five tasks are representative of the changes I made to SMART with respect to the Retail API server functionality.   
 
\subsubsection{Bug fixes and testing} 
 
After the fraud control work was completed I was assigned as product owner of SMART and charged with general maintenance. The InControl development team use a software package called "JIRA" to track bugs within the Dublin office. A typical JIRA report contains the issue or "ticket" number, effected components and release, the steps to reproduce the bug, the bug status, i.e. Fixed, In progress etc., the person who reported the bug and the person assigned to fix it. \todo{Mention that this somewhat exceeded the expectations of what I could do}  
 
\image{jira.jpg}{A JIRA ticket.} 
 
When the maintenance work began SMART had a few outstanding issues. Everyone in QA was more than happy to answer questions and supply feedback as it had been some time since someone had done regular maintenance on SMART. 
The typical work flow for a given bug was: \todo{Replace ";" with ":" in most places} 
\begin{itemize} 
\item Follow the steps to reproduce and determine the nature of the error. 
\item If the issue was determined as a genuine SMART issue, i.e. Not a local configuration issue, log it as fix required. If it was determined to be an issue with the Retail API server no action was taken in JIRA until the problem was found. 
\item Locate where the error was happening in SMART and fix it. If the error was the server it was usually due to one of three reasons. 
\begin{itemize} 
\item The server was released with an incorrect configuration file. 
\item Incomplete server code was checked in. This was my fault for not building SMART against full server releases, usually. 
\item A genuine bug in the server. This was the hardest and most interesting situation to deal with. The Retail API server consists of around 20,000 lines of Java code and a further 10,000 lines of code in the stored procedures. The generated code contains of 120,000, although it can be considered almost entirely error free. Tracking an issue through the server was quite time consuming. First, the call from SMART had to be found and verified to be functioning correctly. The request handler in the server and its related classes then had to be checked, as well as the server configuration. Many requests go through one or more pre-processors before reaching the handler so these also had to be checked. Finally the stored procedure was checked. In the few Retail API issues this is where the majority of problems came from. SQL code can be tricky to debug and errors can result from very subtle changes. Additionally, a typical stored procedure will make calls to other stored procedures and perform operations on a number of tables. 
 
If the bug was confirmed to be an issue with the Retail API server the standard process was to verify it with one of the full time developers and have them log the fix. Only on a few occasions did I commit a fix to the server, and these were very trivial bugs. I was not strictly involved in the Retail API server. It is one of the core InControl products, so the rigor surrounding it is of a far, far greater standard than SMART. 
\end{itemize} 
\item Once the bug was fixed it was re-tested. This usually amounted to going through the steps to reproduce the issue and verifying the resulting behaviour was the expected behaviour, not the behaviour reported in the issue. 
\end{itemize} 
During the months spent maintaining SMART I worked with nearly every member of the QA team to some extent. SMART is a tool they use daily so they were happy to help me in fixing issues as quickly as possible. 
 
\subsubsection{TOM} \todo{Rename this section} 
Along with the maintenance work I received several feature requests from a developer charged with InControl implementations, i.e. Managing the platform in the various environments ( p. \todo{ENVIRONMENTS}). He wanted to use SMART as window into InControl to view overall system health. This resulted in implementing several new requests within the Cronos server, which is ultimately released with a production system. These requests were largely different forms of statistics for different events. Some of these requests were later charted graphically in SMART to give a very quick overview of system health. SMART is not authorized for use within the higher environments \todo{Make sure these are explained} but these charts are still valuable for testing. 
 
\todo{Spent a good bit of time on this stuff, needs more discussion} 
 
\todo{Customer XML also} 
 
\image{smart_chart.png}{A chart showing a history of notifications issued by the InControl system.} 
  
\cite{Something about reusability through modularity} 
 
 
\subsection{OIL} 
\label{OIL} 
Orbiscom Interface Language is an in house Interface Definition Language. It is used to specify how various components communicate with each other. It can be used to specify basic data objects, request definitions and their behaviour. I used OIL extensively throughout the internship. There was some existing documentation that was helpful, but the bulk to the knowledge I acquired was from asking questions and reading the OIL grammar file. The OIL grammar file is a standard Backusâ€“Naur Form grammar and was quite easy to read because BNF grammars were covered extensively in the Discrete Mathematics and Compiler Design courses taken in second and third year respectively. There were some difficulties surrounding the implementation of the OIL compiler. Functionality described in the grammar was not implemented in the compiler. The most prominent being the inability to set default values for request parameters. \todo{Draw more attention to the fact I found this?} 
OIL is a fully fledged language with its own compiler. The compiler generates program code \todo{How is not just a code generator?} that can then be compiled. At the moment, Java and Flex \todo{Correct?} can be generated using OIL. 
OIL is used throughout the InControl Platform. Virtually every component uses it in some respect. Even the components that do not use OIL directly, e.g. do not call the OIL generated code, use OIL indirectly as all of the XML messages are generated by OIL classes. 
During my time at MasterCard I too used OIL extensively. If a new request is needed it must be created using OIL. There are several steps involved, beginning with creating the database code to service the request. The stored database procedure is invoked by and OIL request handler. This handler must be written manually. I wrote a script to automate this process somewhat. The handler is invoked by an OIL generated handler, which is configured to run when a specific request comes in. This request is again an OIL generated type. The steps to add a new request to a component called Cronos follows 
\subsubsection{Create the stored procedure} 
A stored procedure is set of pre-compiled SQL statements stored inside a database. This example is just a simple select statement but any SQL is valid. Stored procedures are saved to \textit{release}/database/procedures/src/o\_\textit{module}.pls, e.g. \textbf{\textit{r12q4/database/procedures/src/o\_cronos.pls}}. \todo{Does that look a bit much}Each stored procedure package consists of two sections, procedure prototype definitions and procedure bodies. This is the same as any programming language, the prototypes are function heads and the implementing code is contained within the body. 
A definition: 
\begin{verbatim} 
PROCEDURE GetIssuerConfig ( 
        p_issuer_id IN VARCHAR2, 
        r_issuer_config OUT Globals.ref_cursor); 
\end{verbatim} 
Here the procedure takes a string and a reference cursor. A cursor acts like a pointer to the result set returned by the query. A ref cursor is similar, the key difference being that it is not bound to a single result set and can also be passed back up to the client, which is somewhat important. If no result set is returned, i.e., an insert, no ref cursor is need. 
The implementing code. This is just standard SQL code 
\begin{verbatim} 
PROCEDURE GetIssuerConfig ( 
        p_issuer_id IN VARCHAR2, 
        r_issuer_config OUT Globals.ref_cursor) 
    IS 
    BEGIN 
        OPEN r_issuer_config FOR 
            SELECT issuer_name, authentication_type, authorisation_type, session_timeout, 
            authenticate_pan_password, allow_change_email_addr, allow_set_user_name, create_date, update_date 
            FROM issuer_config 
            WHERE issuer_id = p_issuer_id; 
    END; 
\end{verbatim} 
The stored procedure must then be loaded into the database. If there are any errors in the stored procedure file loading will fail and the entire file and all of the contained procedure will be rejected. It is generally a good idea to test the procedure works as intended before writing code to access it. It can be executed by specifying the package and procedure name. If the procedure requires a reference cursor it must also be declared 
\begin{verbatim} 
var blah refcursor 
execute 'cronos12q4db.GetIssuerConfig('1', :blah); 
\end{verbatim} 
 
\subsubsection{OIL Interface} 
There are two files used to define the classes OIL generates, the OIL definition file and the OIL procedure definition file. The procedure definition file is nearly identical to the stored procedure definition file, so it may be easier to write it first and then the definition file. 
 
The OIL definition file, e.g. \textbf{\textit{cronos.oil}} is used to define the request and any data types needed by that request. The file begins with an interface definition, this is the package where the generated code is stored. This is optionally followed by any imports needed. This allows OIL files to be split up very easily. Typically, that is, by convention only, data types are defined. The two built in data types are \textbf{String} and \textbf{Number}. Any user defined data types are collections of these and other used defined data types. Default values are supported. This is where the default values mentioned in the Retail API Server section come from. They are specified in the OIL files and are thus hard coded into the generated Java code. 
\begin{verbatim} 
 type IssuerConfig 
    { 
        String IssuerName; 
        Number AuthenticationType; 
        String AuthorisationType; 
        Number SessionTimeout; 
        String AuthenticatePanPassword; 
        String AllowChangeEmailAddr; 
        String AllowSetUserName = "N"; 
    } 
\end{verbatim} 
The request definitions follow the type definitions 
\begin{verbatim} 
request GetIssuerConfig( 
        required String IssuerId) 
    { 
        response 
        { 
            IssuerConfig IssuerConfig; 
        } 
    } 
\end{verbatim} 
This is a very basic example of what OIL is capable of and will result in a basic data model Object and request definition. OIL supports annotations to support various features, such as comments to be included in the generated code and enforcing mandatory members in request parameters etc. Arrays are also supported. They can be restricted to containing elements of a specified type or multiple elements of different specified types. OIL definition files also have access to a Context Object. Values can be stored and retrieved from this Object when the generated code is running. This is particularly useful for large and complex operations. Values can be saved in the context to avoid having to pass them between requests. 
 
The OIL procedure file maps the values returned by the stored procedure to the members of the OIL Object returned in the query response. It is an Object relational mapping similar to technologies like Hibernate \todo{Might be stretching it a bit there} 
\begin{verbatim} 
 procedure GetIssuerConfig[Cronos12q4DB.GetIssuerConfig] ( 
        String IssuerId => p_issuer_id) 
    { 
        output 
        { 
            IssuerConfig IssuerConfig <= r_issuer_config 
            { 
                String IssuerName <= issuer_name; 
                Number AuthenticationType <= authentication_type; 
                String AuthorisationType <= authorisation_type; 
                Number SessionTimeout <= session_timeout; 
                String AuthenticatePanPassword <= authenticate_pan_password; 
                String AllowChangeEmailAddr <= allow_change_email_addr; 
                String AllowSetUserName <= allow_set_user_name; 
            } 
        } 
    }  
\end{verbatim} 
One important thing to note here is that the type definitions declared in the OIL definition file must be imported into the OIL procedures file. Another more subtle issue is the presence of duplicate column names in a result set. The values will always be taken from the first column in the result set. The easiest way I found to fix this was to modify the stored procedure to rename the conflicting columns in the result set. Oracle makes this very easy, the column name need only be appended with the desired name, i.e. \textit{ table\_name.column\_name "desired\_name" } 
 
The OIL compiler is then ran against the files which outputs several classes per request.  The final step is to write a handler. The handler is executed by the server on receiving a particular request, mapped in the config files as outlined on page \pageref{handlerset} 
 
\subsection{OIL Handler} 
An appropriate handler class must be created in order to service the request. The handler must implement OilProcessor, specifically the OIL processor generated above \todo{Hierarchy tree here}. The purpose of the second, generated processor is to enforce parameter types. \todo{Get input}. For simple queries very little is needed. The request parameters are extracted from the incoming request object. The stored procedure is then called with these parameters. The output is put into the response object and sent back to the calling client. 
 
\begin{verbatim} 
package com.orbiscom.cronos.oil; 
 
import java.sql.Connection; 
 
import com.orbiscom.apollo.core.MessageProcessingException; 
import com.orbiscom.atlas.core.ErrorMessage; 
import com.orbiscom.atlas.core.MessageContext; 
import com.orbiscom.atlas.xml.ApplicationHandler; 
import com.orbiscom.cronos.oil.proc.GetIssuerListOutput; 
import com.orbiscom.cronos.oil.proc.GetIssuerListProc; 
import com.orbiscom.oil.db.ProcFactory; 
import com.orbiscom.oil.db.StoredProcedureException; 
 
 
public class GetIssuerListHandler 
	implements GetIssuerListProcessor 
{ 
	private final GetIssuerListProc iProc; 
 
	public GetIssuerListHandler() 
		throws StoredProcedureException 
	{ 
		iProc = ProcFactory.getProc(GetIssuerListProc.class); 
	} 
 
	public void process(MessageContext ctx, GetIssuerListRequest request, GetIssuerListResponse response) 
		throws MessageProcessingException 
	{ 
		try 
		{ 
			Connection conn = ctx.getIssuerDatabaseConnection(ApplicationHandler.APPLICATION_DB); 
 
			GetIssuerListOutput out = iProc.call(ctx, conn); 
 
			response.setIssuers(out.getIssuers()); 
		} 
		catch (MessageProcessingException ex) 
		{ 
			throw ex; 
		} 
		catch (Exception ex) 
		{ 
			throw new ErrorMessage.GeneralError(ex); 
		} 
	} 
} 
\end{verbatim} 
Writing these handlers is particularly tedious. However, as they are usually very similar the process can, and was, automated with a simple script. 
The handler must then be added to the servers handler configuration file as shown on page \pageref{handlerset} 
 
\cite{OIL} 
 
\section{Web Admin Console} 
The second project worked on during the internship was an Issuer Administration web console. The requirement was to build a web application to allow a member of development, customer implementation support etc., to configure a new issuer on the InControl platform. 
 
\subsection{Overview} 
Adding a new issuer to the InControl platform is a complicated process. The database administrator manually inserts the new issuer each time. The majority of the data inserted is static but each new issuer still requires a large amount of custom data. Traditionally the setup is done by modifying an sql script. The issuer specific information is included along with the static data and ran against the relevant databases. This is a time consuming process for the database administrator. There are a number of common steps needed to setup a basic issuer and several optional steps, the use of which is dictated by the issuer and the products they wish to use. As InControl continues to grow more and more new issuers are being brought onto the InControl platform. Manually inserting each new issuer is no longer practical so a faster solution is needed. 
 
  
 
\subsection{Objectives} 
 
The objective of the project was to create a web application to allow issuer setup and maintenance. The application was to be used internally within MasterCard, starting in the lower environments and gradually progressing to use in a production setting. The main objective of the project was to allow issuers to be setup quickly and easy, giving the database administrators more to work on areas other than tedious updates. 
 
Initially it was decided to integrate the new application with the existing Client Support Services web application. CSS is an application released to customers to enable them to support end users, i.e. Card holders. This is a large application, about 180,000 lines of code. Some time was spent investigating the best way to integrate the new application with the existing CSS. Ultimately however, the requirements were changed. A new, standalone application was to be created. This was decided for several reasons. CSS is a product released to issuers to support their users. The issuer setup application was to be used internally at MasterCard, there is no overlap in the user base at all so it makes little sense to integrate the two. The time needed to create a new set of separate functionality with the CSS was also a factor. 
 
It was then decided to create a new application from scratch. There were two phases to this, a prototype to act as a proof of concept and help get the project approved, and a full project to be used. I was to work with another intern on the prototype and eventually the full application. The application was to be built using the Spring Framework using JSP to render the web interface. 
 
The prototype needed to demonstrate core functionality. A series of steps were to be supplied to the user to allow them to input the information need to create a new issuer. They could then review their work in some way. The full project was to expand on this with the inclusion of user rolls, environments and scope to expand the functionality.  
 
\subsection{Description} 
\todo{Needed?} 
 
The static data could be hidden from the user with only the core, issuer specific, information presented for the user to supply. This application would use the Cronos server to insert the data. Cronos contains calls to setup, configure and maintain an issuer. This is done using 60 separate requests, some of which are very substantial. The steps to setup an issuer were already done which simplified the project immensely. 
 
 
 
 
\subsection{Client Support Service} 
 
Initially the CSS web application was to be extended to include issuer setup functionality. It is split into a client and server, with the server making subsequent calls to the various other servers that make up the InControl platform. The client and server make up about 180,000 lines without counting the servers used to processes the requests and their underlying stored procedures. As such, CSS is quite a substantial project, and where to integrate a completely separate set of functionality was not obvious. 
CSS is implemented in Java and user Java Server Faces. A few weeks were spent, in addition to working on SMART, researching and experimenting with JSF and CSS. Ultimately little was achieved technically.  \todo{More? Find notes on JSF} 
 
\subsection{Prototype} 
 
The prototype web application was written using the spring framework and the existing Cronos requests. The prototype was to implement one specific use case, the creation of a new issuer for the IPC fraud control product. 
Spring was chosen because several of the developers within InControl were familiar with it, it is approved for use within MasterCard and is one the most popular inversion of control containers for Java. 
This project was very different from working on SMART. It gave me exposure to the full software development life-cycle in a scaled down manner. The functional specification above was finalized and a collection of documents needed to achieve this were gathered. At the core of this was the existing issuer database setup script and Cronos API requests, as well as some customer documentation used to explain the meaning for specific values, allowing them to be grouped logically in the application. 
Using the script and the Cronos source code, a series of 10 steps were drawn up. The first 6 were concerned with creating the basic issuer, with the remaining used to add a bin range and add the fraud control product. 
 
\subsubsection{Spring} 
 
Spring is an enterprise Java framework built around a single core principle, dependency injection. Dependency injection is a solution to the object coupling problem. If a number of objects make direct calls to each other they are said to be tightly coupled. Removing or changing any of them will cause the whole system to break. When implemented, this type of design quickly leads to large amounts of code that cannot be touched and extensive code duplication. The classic example of this is some sort of client server application. A client fetches data through a file reader, the server. The client implementation can only fetch data in one way. If the server is changed to another source, a database or a socket, the client code must be re-factored.  
 
Object oriented design mitigates this but does not completely solve the issue. Code written against interfaces allows functionality to be changed as long the interface contract remains the same. The problem a reference to the implementing class must still be hardwired into the code. In the example above, the server would implement an interface. The interface would define standard behaviour need by the client. The implementation details are left up to the concrete server implementation. This concrete implementation still needs be instantiated somewhere in code. While this is a good solution to the problem, code still needs to be re-factored if the data-source changes. 
 
Dependency injection, or Inversion of Control principle, aims to solve this problem by changing the way a program executes. Normally when a program starts some main function is called, the dependencies are created and execution proceeds. Inversion of Control does the reverse. All of the dependences and relationships are create by the IoC container and injected into the main program as properties.  In the example above, server implementation to be injected can be configured without having to refactor any code. 
 
Spring is a very large framework with a wide range of functionality, but Inversion of Control is the principle on which it is built. 
 
Until Spring version three, all configuration was done using XML. This allows for greater flexibility without having to recompile code but essentially splits development in two. In addition to writing code, various properties have to be created in separate XML files which slows development. Spring version three supports annotations. The properties traditionally stored in XML files can now be written alongside Java code. This helps to keep everything together and makes code easier to read. However, if a property is changed, code refactoring is needed. The solution lies somewhere in the middle. Once the properties have been finalized, they can be moved put to XML files. 
 
\cite{Spring} 
 
\subsubsection{Design and Implementation} 
 
The web application was to an MVC structure. The Spring framework has its own MVC web application framework, Spring MVC. The framework defines strategy interfaces for all of the responsibilities which must be handled by a requests-based framework. For each layer within the MVC structure, an interface is defined. The various components of the MVC implementation can then be swapped out at runtime using dependency injection. The basic structure is show below; 
 
\image{spring_mvc}{Overview of Spring MVC design \cite{SpringMVC}} 
 
 
 
\begin{itemize} 
\item Model: Plain old Java objects. Spring supports object relational mapping using technologies like hibernate. If this is used, the model objects are declared as "Entity". The class name must map to the underlying table in the database and its members must map to the columns. All member access is done through appropriate accessor methods. 
OIL was to be used for database access in this project, as the bulk of the logic was contained within the stored procedures. The OIL generated classes supplied the model objects. OIL generated objects are not traditional Java objects (p. \pageref{metadata}) but were full inter-operable with Spring. Although the members of an OIL class are contained within a map, they can still be accessed using specific accessor methods. 
There were also some custom model object specific to the application. Many of the parameters in a typical Cronos request had sensible default values so it did not make sense to present these to the user. Thus, the model objects the user populated in the views were logical groupings of parameters used in several requests, i.e. The issuer name and email address are set in separate requests but it makes sense to have the user supply the information on the same screen. A model object was created to encapsulate this screen and its various members were then used to populate several requests to Cronos. 
\item DAO: The data access layer interacts with the application data. In the majority of cases this is an sql database but it can be any sort of data source. The DAO in this application sent requests to the Cronos server, which in turn interacted with the database. This became the most substantial component of the application as the bulk of the business logic had already been implemented in stored procedures. It was decided to continue to use Cronos to access these stored procedures for several reason. Cronos utilizes the Apollo framework which is very well understood within InControl. It was decided that given the collective amount of experience with Apollo, it would be far quicker to build the application using it. Whether it did or not is debatable, but if the product was to be developed fully it would be very hard to justify not using Apollo. 
\item Service Layer: The service layer is where Spring begins to depart slightly from a traditional MVC structure. A traditional MVC structure use the Model to represent the data being manipulated by the system, the View to present this data to the user and the Controller to do everything else. The service layer in Spring is used to house the business logic of an application, leaving the controller free to parse user input, delegate the appropriate action and return the response, or in other words to control the application. The business logic in a large application, although not in this case, can be substantial, so it makes sense to segment it. In this application the business logic was contained within Cronos, so the service layers function was simple to give the controller and interface into the DAO. 
\item Controller: As above, the controller is responsible for parsing input and delegating the appropriate action. It passes this sanitized data to the service layer which then makes calls into the DAO. The flow of the various steps needed to setup an issuer was defined in the controllers, as these classes are responsible for displaying the various views in addition pulling data from them. 
\item View: As per traditional MVC, the view is responsible for allowing the user to interact with the model. Spring supports a number of different technologies to create the view. Java Server Pages were used, they are the go to technology used to create dynamic pages with Java and are very well supported by Spring. JSP, along with Java Server Faces, is also employed by the majority of the web applications developed within InControl. 
\end{itemize} 
 
The Spring web application is then hosted inside a Java servlet container, Apache Tomcat was used in this instance. 
 
The controllers and DAO implementation took the majority of implementation time. Each page in the application has its own controller. Some of these controllers are quite small but it made sense to segment them by page.  
 
\begin{verbatim} 
@Controller 
@SessionAttributes 
@RequestMapping("bin_setup") 
public class BinRangeController 
{ 
 
    private static final IssuerConsoleLogger LOGGER = IssuerConsoleLogger.getInstance(BinRangeController.class); 
 
    @Autowired 
    private IssuerService issuerService; 
 
    @RequestMapping(method = RequestMethod.GET) 
    public String showPage(Map model, HttpSession session) 
    { 
        LOGGER.debug("<<< /bin_setup.htm GET >>> called"); 
 
        model.put("binrange", new InitialBinRange()); 
        return "bin_setup"; 
    } 
 
    @RequestMapping(method = RequestMethod.POST) 
    public String processForm(@Valid InitialBinRange binrange, BindingResult result, HttpSession session) 
    { 
        LOGGER.debug("<<< /bin_setup.htm POST >>> called"); 
 
        Issuer issuer = (Issuer) session.getAttribute("issuer"); 
        issuerService.setupIssuer(issuer, binrange); 
        return "redirect:issuer_list.htm"; 
    } 
} 
 
 
\end{verbatim} 
 
The above code snippet gives an example of a variety of Spring functionality.  
 
@Controller: Spring does not enforce the MVC model structurally, i.e. y package name. Classes are configured to be part of the various layers. This can be done in an XML file, or in this example with an annotation.  
 
@SessionAttributes: The lifetime of an object can also be configured. In this example the object expires after the user disconnects. Object lifetime can be set anywhere from a single page display to the entire application lifetime. 
 
@RequestMapping: Requests can be mapped to classes or individual methods. This class is configured to process "issuer_details" requests. The request type is specified in the request line of the HTTP request. 
 
\label{aspects} 
The logger is an example of Aspect Oriented Programming. Aspect Oriented Programming complements Object Oriented programming by allowing the separation of cross cutting concerns. Logging is the classic example of a cross cutting concern because the logging strategy effects every part of the system. By encapsulating logging as an aspect when can make changes to its behaviour without having to impact the rest of the system. 
\cite{AOP} 
 
@Autowired: This annotation tells Spring to inject an object at run time. In this application, IssuerService is an interface. The implementing class is loaded automatically and injected by the IoC container. As stated above, this is a very powerful technique but can make the code a little confusing. It is not immediately obvious what implementing class is being used. Here there is only one implementing class so the choice is easy. If there are multiple implementations the one to be used must be specified. This indirection can make projects harder to read and debug. 
 
Different methods are invoked depending of the type of incoming request. Before the page can be shown to the user, the model must be populated for the view to interact with. A new InitialBinRange object is inserted into the model and "bin-setup" is returned. This particular Spring application is configured to display the page with the same name as the string returned by the controller. The model and user session need only be declared as parameters and Spring automatically injects them. 
 
Post requests are handed in a similar method. The user has, using the view, populated the model object supplied by the Get request. This object is supplied with the post request. As this is the last step it must be committed to the database. The model object populated in a previous view are extracted from the session and passed down to the service layer.  
 
@Valid: The @Valid annotation in the processForm method declares that the object passed in should first be validated. This can be done by writing a custom validation object or by simply annotating the various members of the model object with constraints.  
 
The model objects are passed from the service layer into the DAO layer in the same manner as displayed above. The DAO implementation is resolved at runtime. The members of the model objects are then used to populate Cronos requests. The DAO implementation is quite long and tedious. It simple populates various Cronos request objects and sends them to the Cronos server. The order of these requests turned out to be quite important. Certain steps in the issuer setup must be taken in a certain order. This sequence was inferred from the DBAs scripts. Cronos calls with appropriate functionality were then found and sent in the proper sequence.  
 
There was also a small bug in Apollo. Apollo allows multiple requests to be sent at once, wrapped inside a "requestor". If any one of these requests fails, none of the requests in the requestor are committed. In theory each requestor should have its own scope, so each request is exposed to the changes made in previous requests. This was not the case, so requestors had to be broken up to allow requests to be committed properly. 
 
After the setup steps had been completed the user was forwarded to a screen to allow them to review their work. This was a simple listing of issuers in the system. The bin ranges associated with an issuer could then be viewed. This was a small feature simply to show that the application could also be used to view system information. 
 
\subsubsection{Completion} 
 
After the prototype had been completed it was shown as part of a pitch to secure funding for a fully developed version. An easy way to set up new issuer on the InControl Platform was something the team had wanted for a number of years. An independent report recently conducted by Computer Science Corporation had highlighted the need for automation of the process. The idea behind the prototype was to show the business leaders in charge of InControl what such an application might look like and that it would not require a significant development effort. The project was approved and given funding for the first quarter of 2013.  
 
\subsection{Further development} 
 
After the project had been approved work began on a more robust version. Unfortunately, a formal specification was not drawn up during the remainder of the internship. I was to target a single use case but build the application to allow for extension. Additional set up use cases, information retrieval and user roles and levels of access were to be added in later. 
 
These requirements were very broad and difficult to account for. After researching user roles and access privileges in Spring I was satisfied that they could be integrated as an aspect (p. \pageref{aspect}) later in development. This left me to concentrate on building the application to allow for later extension. In order for the application to be used and supported in the future it had to be as maintenance free as possible. Changes to the issuer setup process should not require code refactoring. After working with SMART it was understood that such an approach was feasible. The principle of dependency injection employees a similar idea to the dynamic loading of requests in SMART, although it is \textbf{substantially} more complicated. 
 
\subsubsection{Parser} 
The desire was to build a dynamic model that could be configured with XML. The sequence of steps would be specified in an XML file, populated with data where appropriate. A similar project had recently been completed within InControl, the Cronos Batch Utility. This was an extension of the Cronos server that allowed mass registration of issuers. The sequence of requests were specified in an XML file and the data to populate them was contained in flat files. The batch process "digests" the data in these flat files and sends them to the server. However, the Cronos Batch Utility differed in two key areas to the intended functionality of the Issuer Setup application. 
\begin{itemize} 
\item The Cronos Batch Utility populated the XML with data from the flat files and sent the resulting XML directly. Request objects were not created. 
\item The XML messages were not sent over a network, rather they were passed directly to the handlers. This meant the Cronos server had to be running on the machine, not remotely. While the Apollo framework does support sending XML directly, there is substantially more effort involved as opposed to just sending request objects via a requestor.  
\end{itemize} 
The Issuer Setup web application had to be able to send requests remotely, so a different approach was required. The templates from the Cronos Batch Utility were used as base, the idea being that these would be maintained and thus the setup process used in the web application would be maintained. Some slight changes were made. Each setup process was contained within a "wizard", and each "wizard" contained a series of "steps". The requests were contained within these "steps". The requests were separated logically into steps, the idea being that the interface could be generated off the XML at a later date. A summarized version of the resulting XML is shown below: 
\begin{verbatim} 
<Wizards> 
 
    <ComplexTypes> 
        <Type Name="Property" Path="com.orbiscom.atlas.oil"/> 
    </ComplexTypes> 
 
    <Wizard Name="Add Issuer"> 
 
            <Step Name="Add Issuer"> 
            <AddIssuerRequest IssuerId="{IssuerId}" IssuerName="{IssuerName}" /> 
 
            <SetIssuerPropertyRequest IssuerId="{IssuerId}"> 
                <Property Name="SecurityManager" Value="{Cipher}" /> 
            </SetIssuerPropertyRequest> 
        </Step> 
		 
        <Step Name="Add groups"> 
            <AddPhoenixIssuerGroupRequest GroupTag="{GroupName}" /> 
            <AddPhoenixIssuerGroupIssuerIdRequest IssuerId="{IssuerId}" GroupId="0" />	 
        </Step> 
	 
    </Wizard> 
 
</Wizards> 
\end{verbatim} 
 
A parser was than constructed to generate OIL request objects. The resulting parser was relatively complicated, taking several factors into account. It is more complex than the requests creator implemented in SMART, the biggest differences are outlined below. 
 
OIL requests can contain complex and simple types. The parser must be made aware of any complex types so the corresponding classes can be instantiated. After this is done the requests are parsed. First the request name is parsed and the corresponding request object is created using the same approach as SMART \todo{Pageref this}. The request objects attributes are then compared against the attributes specified in the XML. If they match, the XML attribute is parsed and stored in the request object. There are a number of possible event relating to this. 
 
The attributes are not matched by member name, rather the class name set in the attributes meta-data map is compared to the attribute name in the XML. This is because lists of attributes are pluralized by the OIL compiler, i.e. A list of "Property" objects is given the key "Properties" in the OIL object member map. Whether or not a member is in fact a list of members is determined again from the meta-data map by examining the container attribute of the meta-data. A code snippet generated by the OIL compiler is shown below, demonstrating the difference between a single attribute and a list of attributes. 
 
\begin{verbatim} 
cMetaData.put("IssuerId", tmpMetaData = new MetaData("IssuerId", String.class, null, null, null, null)); 
cMetaData.put("Properties", tmpMetaData = new MetaData("Properties", Property.class, List.class, null, null, null)); 
\end{verbatim} 
 
After the nature of the attribute is determined it is populated. There are two possibilities; if a value is specified, as with "GroupId" above, the value is parsed and stored inside the OIL object. If the value specified is of the form "{" <ident> "}" the identifier is stored as a constraint within the OIL object. This is so values can be used to populate multiple requests after the user has specified them once. In the above example, IssuerId is used in multiple requests. It does not make sense to have the user supply IssuerId repeatedly. When they supply it for the first request, the value is indexed by the key "IssuerId". This value can then be inserted into the remaining attributes indexed by "IssuerId". This functionality was not completed, the finished web application will have to contain logic to use the same values across multiple requests, but the identifiers are in place within the model. \todo{Replace with KeyFieldName if time allows} This type of functionality was built into OIL when it was originally designed. The meta-data for every OIL object contains a field for such an identifier. 
 
The result is a relatively elaborate data structure containing a number of request objects: 
 
\begin{verbatim} 
Add Issuer 
	Add Issuer 
		{_BeanType=AddIssuerRequest, IssuerId=null, IssuerName=null} 
		{_BeanType=SetIssuerPropertyRequest, IssuerId=null, Property={_BeanType=Property, Name=SecurityManager, Value=null}} 
	Add groups 
		{_BeanType=AddPhoenixIssuerGroupRequest, GroupTag=null} 
		{_BeanType=AddPhoenixIssuerGroupIssuerIdRequest, IssuerId=null, GroupId=0} 
\end{verbatim} 
 
The data structure is: 
 
\begin{verbatim} 
LinkedHashMap <String,  
    LinkedHashMap <String, ArrayList 
        <Request> 
    > 
> 
\end{verbatim} 
 
Linked hash-maps are used so the ordering of steps is preserved. The first hash-map contains all the different setup "wizards". Each wizard is in turn a hash-map containing a series of "steps". Each step contains an array list of requests. 
\subsubsection{Error Handling} 
 
Writing the XML file is quite an error prone process. There is no tolerance for errors naming objects as these names are used to create objects. Exception handling within the parser had to be robust and return specific errors to allow mistakes to be easily diagnosed. After some reading up on best practices I am confident that the parser is relatively robust. 
\cite{http://www.wikijava.org/wiki/10_best_practices_with_Exceptions Is it appropriate to link a wiki?} 
 
\section{Conclusion} 
Technical Conclusion. Goals. BCS goals. 
\section{References} 
 
 
 
\section{Appendices} 
 
\bibliographystyle{ieeetr} 
\bibliography{bibliography} 
 
\end{document} 
